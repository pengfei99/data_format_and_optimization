{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data format overview\n",
    "## In this tutorial, we will overview evaluate the following data formats\n",
    "1. avro (structured)\n",
    "2. csv (semi-structured)\n",
    "3. json (semi-structured)\n",
    "4. orc (structured)\n",
    "5. parquet (structured) \n",
    "\n",
    "## We evaluate the data formats via:\n",
    "1. Disk usage\n",
    "2. Read/Write latency\n",
    "3. Random data lookup\n",
    "4. Filtering/GroupBy(column-wise)\n",
    "5. Distinct(row-wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Key': 'pengfei/pengfei_test',\n",
       " 'name': 'pengfei/pengfei_test',\n",
       " 'type': 'directory',\n",
       " 'Size': 0,\n",
       " 'size': 0,\n",
       " 'StorageClass': 'DIRECTORY'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import s3fs\n",
    "endpoint = \"https://\"+os.environ['AWS_S3_ENDPOINT']\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': endpoint})\n",
    "event_log_path=\"pengfei/spark-history\"\n",
    "\n",
    "fs.touch('s3://'+event_log_path+'/.keep')\n",
    "fs.info('pengfei/pengfei_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "       .builder.master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "       .appName(\"Python Spark SQL basic example\") \\\n",
    "       .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:master\") \\\n",
    "       .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "       .config(\"spark.executor.instances\", \"5\") \\\n",
    "       .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "       .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "       .config(\"spark.eventLog.dir\",\"s3a://\"+event_log_path) \\\n",
    "       .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.0.1\") \\\n",
    "       .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data_path=\"s3a://pengfei/sspcloud-demo/data_format/netflix.json\"\n",
    "parquet_data_path=\"s3a://pengfei/sspcloud-demo/data_format/netflix.parquet\"\n",
    "avro_data_path=\"s3a://pengfei/sspcloud-demo/data_format/netflix.avro\"\n",
    "orc_data_path=\"s3a://pengfei/sspcloud-demo/data_format/netflix.orc\"\n",
    "csv_data_path=\"s3a://pengfei/sspcloud-demo/data_format/netflix.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The read function read the source data file and convert it to a spark data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_stats(line):\n",
    "    path=\"../tmp/mystats.csv\"\n",
    "    file1 = open(path,\"a\")\n",
    "    file1.write(line+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def read(fmt):\n",
    "    start = time.time()\n",
    "    if fmt == \"json\":\n",
    "        sdf = spark.read.option(\"header\", \"true\").json(json_data_path)\n",
    "    elif fmt == \"csv\":\n",
    "        sdf = spark.read.option(\"header\", \"true\").csv(csv_data_path)\n",
    "    elif fmt == \"avro\":\n",
    "        sdf = spark.read.format(\"avro\").option(\"header\", \"true\").load(avro_data_path)\n",
    "    elif fmt == \"parquet\":\n",
    "        sdf = spark.read.option(\"header\", \"true\").parquet(parquet_data_path)\n",
    "    elif fmt == \"orc\":\n",
    "        sdf = spark.read.orc(orc_data_path)\n",
    "    sdf.show(5,False)\n",
    "    stats=\"{}, {}, {}\".format(fmt, \"read\", time.time() - start)\n",
    "    write_stats(stats)\n",
    "    print(stats)\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The get_shape function prints the shape(e.g. row number and column number) of the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(df,fmt):\n",
    "    start = time.time()\n",
    "    row_num=df.count()\n",
    "    col_num=len(df.columns)\n",
    "    stats=\"{}, {}, {}\".format(fmt, \"get_shape\", time.time() - start)\n",
    "    write_stats(stats)\n",
    "    print(\"The data frame has {} rows and {} columns\".format(row_num,col_num))\n",
    "    print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The stats function prints the min, max and numbers of a column of the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(df,fmt, field=\"rating\"):\n",
    "    start = time.time()\n",
    "    max=df.agg({field: \"max\"})\n",
    "    min=df.agg({field: \"min\"})\n",
    "    count=df.agg({field: \"count\"})\n",
    "    min.show(5,False)\n",
    "    max.show(5,False)\n",
    "    count.show(5,False)\n",
    "    stats=\"{}, {}, {}\".format(fmt, \"stats\", time.time() - start)\n",
    "    write_stats(stats)\n",
    "    print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The random_batch function randomly select rows from the data frame. It can evaluate the ability of random data lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(df,fmt):\n",
    "    start = time.time()\n",
    "    result=df.sample(False, 0.05).collect()\n",
    "    stats=\"{}, {}, {}\".format(fmt, \"random_batch\", time.time() - start)\n",
    "    write_stats(stats)\n",
    "    print(stats)\n",
    "   # return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The distinct function count distinct rows of the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct(df,fmt):\n",
    "    start = time.time()\n",
    "    result = df.distinct().count()\n",
    "    stats=\"{}, {}, {}\".format(fmt, \"distinct\", time.time() - start)\n",
    "    write_stats(stats)\n",
    "    print(stats)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The group_by function group and count the data frame by a specific column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by(df,fmt):\n",
    "    start = time.time()\n",
    "    result=df.groupBy(\"rating\").count()\n",
    "    result.show(5,False)\n",
    "    stats=\"{}, {}, {}\".format(fmt, \"group_by\", time.time() - start)\n",
    "    write_stats(stats)\n",
    "    print(stats)\n",
    "    #return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The filtering function filter data by using a specific boolean condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering(df, fmt, date=\"2005-11-15\"):\n",
    "    start = time.time()\n",
    "    result = df.filter(df.date > date).count()\n",
    "    stats=\"{}, {}, {}\".format(fmt, \"filtering\", time.time() - start)\n",
    "    write_stats(stats)\n",
    "    print(stats)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CSV format evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+\n",
      "|user_id|rating|date      |\n",
      "+-------+------+----------+\n",
      "|1488844|3     |2005-09-06|\n",
      "|822109 |5     |2005-05-13|\n",
      "|885013 |4     |2005-10-19|\n",
      "|30878  |4     |2005-12-26|\n",
      "|823519 |3     |2004-05-03|\n",
      "+-------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "csv, read, 0.8125045299530029\n"
     ]
    }
   ],
   "source": [
    "csv_df=read(\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data frame has 24058262 rows and 3 columns\n",
      "csv, get_shape, 9.196694374084473\n"
     ]
    }
   ],
   "source": [
    "get_shape(json_df,\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|min(rating)|\n",
      "+-----------+\n",
      "|1          |\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|max(rating)|\n",
      "+-----------+\n",
      "|5          |\n",
      "+-----------+\n",
      "\n",
      "+-------------+\n",
      "|count(rating)|\n",
      "+-------------+\n",
      "|24053764     |\n",
      "+-------------+\n",
      "\n",
      "csv, stats, 36.65289235115051\n"
     ]
    }
   ],
   "source": [
    "# get min, max and row number of column rating\n",
    "stats(json_df,\"csv\",field=\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv, random_batch, 17.773473978042603\n"
     ]
    }
   ],
   "source": [
    "random_batch(json_df,\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv, distinct, 21.709513187408447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12168704"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct(json_df,\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|rating|count  |\n",
      "+------+-------+\n",
      "|3     |6904181|\n",
      "|null  |4498   |\n",
      "|5     |5506583|\n",
      "|1     |1118186|\n",
      "|4     |8085741|\n",
      "+------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "csv, group_by, 12.347777128219604\n"
     ]
    }
   ],
   "source": [
    "group_by(json_df,\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv, filtering, 10.43360447883606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "850269"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtering(json_df,\"csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Json format evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------+\n",
      "|date      |rating|user_id|\n",
      "+----------+------+-------+\n",
      "|2005-09-06|3     |1488844|\n",
      "|2005-05-13|5     |822109 |\n",
      "|2005-10-19|4     |885013 |\n",
      "|2005-12-26|4     |30878  |\n",
      "|2004-05-03|3     |823519 |\n",
      "+----------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "json, read, 9.966949701309204\n"
     ]
    }
   ],
   "source": [
    "json_df=read(\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_shape(json_df,\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|min(rating)|\n",
      "+-----------+\n",
      "|1          |\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|max(rating)|\n",
      "+-----------+\n",
      "|5          |\n",
      "+-----------+\n",
      "\n",
      "+-------------+\n",
      "|count(rating)|\n",
      "+-------------+\n",
      "|24053764     |\n",
      "+-------------+\n",
      "\n",
      "json, stats, 35.458019971847534\n"
     ]
    }
   ],
   "source": [
    "# get min, max and row number of column rating\n",
    "stats(json_df,\"json\",field=\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json, random_batch, 17.13827896118164\n"
     ]
    }
   ],
   "source": [
    "random_batch(json_df,\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json, distinct, 21.003305673599243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12168704"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct(json_df,\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|rating|count  |\n",
      "+------+-------+\n",
      "|3     |6904181|\n",
      "|null  |4498   |\n",
      "|5     |5506583|\n",
      "|1     |1118186|\n",
      "|4     |8085741|\n",
      "+------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "json, group_by, 12.402800798416138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[rating: string, count: bigint]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_by(json_df,\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json, filtering, 11.020655632019043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "850269"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtering(json_df,\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Avro format evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_df=read(\"avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Parquet format evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+\n",
      "|user_id|rating|date      |\n",
      "+-------+------+----------+\n",
      "|1488844|3     |2005-09-06|\n",
      "|822109 |5     |2005-05-13|\n",
      "|885013 |4     |2005-10-19|\n",
      "|30878  |4     |2005-12-26|\n",
      "|823519 |3     |2004-05-03|\n",
      "+-------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "parquet, read, 1.6400139331817627\n"
     ]
    }
   ],
   "source": [
    "parquet_df=read(\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data frame has 24058262 rows and 3 columns\n",
      "parquet, get_shape, 1.010782241821289\n"
     ]
    }
   ],
   "source": [
    "get_shape(parquet_df,\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet, random_batch, 6.159161567687988\n"
     ]
    }
   ],
   "source": [
    "random_batch(parquet_df,\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|min(rating)|\n",
      "+-----------+\n",
      "|1          |\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|max(rating)|\n",
      "+-----------+\n",
      "|5          |\n",
      "+-----------+\n",
      "\n",
      "+-------------+\n",
      "|count(rating)|\n",
      "+-------------+\n",
      "|24053764     |\n",
      "+-------------+\n",
      "\n",
      "parquet, stats, 4.589794635772705\n"
     ]
    }
   ],
   "source": [
    "stats(parquet_df,\"parquet\",field=\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet, distinct, 164.0264663696289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12168704"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct(parquet_df,\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|rating|count  |\n",
      "+------+-------+\n",
      "|3     |6904181|\n",
      "|null  |4498   |\n",
      "|5     |5506583|\n",
      "|1     |1118186|\n",
      "|4     |8085741|\n",
      "+------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "parquet, group_by, 1.5566697120666504\n"
     ]
    }
   ],
   "source": [
    "group_by(parquet_df,\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet, filtering, 1.3744680881500244\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "850269"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtering(parquet_df,\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. ORC format evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+\n",
      "|user_id|rating|date      |\n",
      "+-------+------+----------+\n",
      "|1488844|3     |2005-09-06|\n",
      "|822109 |5     |2005-05-13|\n",
      "|885013 |4     |2005-10-19|\n",
      "|30878  |4     |2005-12-26|\n",
      "|823519 |3     |2004-05-03|\n",
      "+-------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "orc, read, 2.3085367679595947\n"
     ]
    }
   ],
   "source": [
    "orc_df=read(\"orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data frame has 24058262 rows and 3 columns\n",
      "orc, get_shape, 1.4532253742218018\n"
     ]
    }
   ],
   "source": [
    "get_shape(orc_df,\"orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orc, random_batch, 6.4762678146362305\n"
     ]
    }
   ],
   "source": [
    "random_batch(orc_df,\"orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|min(rating)|\n",
      "+-----------+\n",
      "|1          |\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|max(rating)|\n",
      "+-----------+\n",
      "|5          |\n",
      "+-----------+\n",
      "\n",
      "+-------------+\n",
      "|count(rating)|\n",
      "+-------------+\n",
      "|24053764     |\n",
      "+-------------+\n",
      "\n",
      "orc, stats, 4.612210035324097\n"
     ]
    }
   ],
   "source": [
    "stats(orc_df,\"orc\",field=\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orc, distinct, 185.58755350112915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12168704"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct(orc_df,\"orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|rating|count  |\n",
      "+------+-------+\n",
      "|3     |6904181|\n",
      "|null  |4498   |\n",
      "|5     |5506583|\n",
      "|1     |1118186|\n",
      "|4     |8085741|\n",
      "+------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "orc, group_by, 1.680478811264038\n"
     ]
    }
   ],
   "source": [
    "group_by(orc_df,\"orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orc, filtering, 1.186652421951294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "850269"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtering(orc_df,\"orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop the spark cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop sparksession\n",
    "spark.sparkContext.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the spark cluster is well closed. You should not see any python-spark pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                     READY   STATUS      RESTARTS   AGE\n",
      "deleting-pods-with-completed-status-1615298400-jk8c6     0/1     Completed   0          51m\n",
      "jupyter-1615290850-69d9fbd8d6-rlwhs                      1/1     Running     0          177m\n",
      "python-spark-sql-basic-example-ca8c007817777a02-exec-1   1/1     Running     0          84s\n",
      "python-spark-sql-basic-example-ca8c007817777a02-exec-2   1/1     Running     0          84s\n",
      "python-spark-sql-basic-example-ca8c007817777a02-exec-3   1/1     Running     0          84s\n",
      "python-spark-sql-basic-example-ca8c007817777a02-exec-4   1/1     Running     0          84s\n",
      "python-spark-sql-basic-example-ca8c007817777a02-exec-5   1/1     Running     0          84s\n",
      "ubuntu-1612965548-79c9567b44-nhs9p                       1/1     Running     0          27d\n"
     ]
    }
   ],
   "source": [
    "! kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
