{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data format overview\n",
    "## 1.0 Introduction\n",
    "In this tutorial, we will overview and evaluate the following data formats\n",
    "1. avro (structured)\n",
    "2. csv (semi-structured)\n",
    "3. json (semi-structured)\n",
    "4. orc (structured)\n",
    "5. parquet (structured) \n",
    "\n",
    "We evaluate the data formats by measuring the latency of the following data operations :\n",
    "1. Disk usage\n",
    "2. Read/Write latency\n",
    "3. Random data lookup\n",
    "4. Filtering/GroupBy(column-wise)\n",
    "5. Distinct(row-wise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important notes:\n",
    "1. To avoid erreur, you need to execute the code block one by one from head to tail.\n",
    "2. Some code block need to be modified. If you see a comment \"#### Modify\", you need to follow the instruction to modify the default value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Spark environment configuration\n",
    "1. Create a spark session for running spark SQL operations\n",
    "2. Setup a spark history server for monitoring and logging spark operations after the spark operations end.\n",
    "\n",
    "In this section you need to modify the **bucket_name** value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Key': 'pengfei/tmp/spark-history',\n",
       " 'name': 'pengfei/tmp/spark-history',\n",
       " 'type': 'directory',\n",
       " 'Size': 0,\n",
       " 'size': 0,\n",
       " 'StorageClass': 'DIRECTORY'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import s3fs\n",
    "endpoint = \"https://\"+os.environ['AWS_S3_ENDPOINT']\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': endpoint})\n",
    "\n",
    "#### Modify, you need to change the bucket_name to your own minio bucket name \n",
    "bucket_name=\"pengfei\"\n",
    "\n",
    "event_log_path=\"{}/tmp/spark-history\".format(bucket_name)\n",
    "fs.touch('s3://'+event_log_path+'/.keep')\n",
    "fs.info(event_log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://kubernetes.default.svc/api/v1/namespaces/user-pengfei/configmaps. Message: Forbidden! User user doesn't have permission. configmaps is forbidden: User \"system:serviceaccount:user-pengfei:jupyter-1620304224\" cannot create resource \"configmaps\" in API group \"\" in the namespace \"user-pengfei\".\n\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:589)\n\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:526)\n\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:492)\n\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:451)\n\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:252)\n\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:879)\n\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:341)\n\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:84)\n\tat org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend.setUpExecutorConfigMap(KubernetesClusterSchedulerBackend.scala:77)\n\tat org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend.start(KubernetesClusterSchedulerBackend.scala:99)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:220)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:579)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8b44bd12cdd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.eventLog.enabled\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.eventLog.dir\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"s3a://\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mevent_log_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m        \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.jars.packages\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"org.apache.spark:spark-avro_2.12:3.0.1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m        \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                         \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m--> 147\u001b[0;31m                           conf, jsc, profiler_cls)\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \"\"\"\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1569\u001b[0;31m             answer, self._gateway_client, None, self._fqn)\n\u001b[0m\u001b[1;32m   1570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1571\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://kubernetes.default.svc/api/v1/namespaces/user-pengfei/configmaps. Message: Forbidden! User user doesn't have permission. configmaps is forbidden: User \"system:serviceaccount:user-pengfei:jupyter-1620304224\" cannot create resource \"configmaps\" in API group \"\" in the namespace \"user-pengfei\".\n\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:589)\n\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:526)\n\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:492)\n\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:451)\n\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:252)\n\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:879)\n\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:341)\n\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.create(BaseOperation.java:84)\n\tat org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend.setUpExecutorConfigMap(KubernetesClusterSchedulerBackend.scala:77)\n\tat org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend.start(KubernetesClusterSchedulerBackend.scala:99)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:220)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:579)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "       .builder.master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "       .appName(\"Evaluate data format\") \\\n",
    "       .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:master\") \\\n",
    "       .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "       .config(\"spark.executor.instances\", \"5\") \\\n",
    "       .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "       .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "       .config(\"spark.eventLog.dir\",\"s3a://\"+event_log_path) \\\n",
    "       .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.0.1\") \\\n",
    "       .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Setup data path\n",
    "The data source is from https://www.kaggle.com/netflix-inc/netflix-prize-data?select=combined_data_1.txt. For saving some time, we just covert it to different format and uploaded it to our data lake. Because writing data takes much more time than reading. But we recoreded the latence of write operations, so we can analyze them after.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data_path=\"s3a://pengfei/diffusion/data_format/netflix.json\"\n",
    "parquet_data_path=\"s3a://pengfei/diffusion/data_format/netflix.parquet\"\n",
    "avro_data_path=\"s3a://pengfei/diffusion/data_format/netflix.avro\"\n",
    "orc_data_path=\"s3a://pengfei/diffusion/data_format/netflix.orc\"\n",
    "csv_data_path=\"s3a://pengfei/diffusion/data_format/netflix.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Some useful functions for evaluating data format\n",
    "\n",
    "In this section, we define some useful functions which we will use after. You don't need to understand them to finish this tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 The read function read the source data file and convert it to a spark data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path for storing operation stats\n",
    "data_format_op_stats_path=\"../tmp/op-stats.csv\"\n",
    "\n",
    "# file path for storing size stats\n",
    "data_format_size_stats_path=\"../tmp/size-stats.csv\"\n",
    "def write_stats(line):\n",
    "    file1 = open(data_format_op_stats_path,\"a\")\n",
    "    file1.write(line+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def read(fmt):\n",
    "    start = time.time()\n",
    "    if fmt == \"json\":\n",
    "        sdf = spark.read.option(\"header\", \"true\").json(json_data_path)\n",
    "    elif fmt == \"csv\":\n",
    "        sdf = spark.read.option(\"header\", \"true\").csv(csv_data_path)\n",
    "    elif fmt == \"avro\":\n",
    "        sdf = spark.read.format(\"avro\").load(avro_data_path)\n",
    "    elif fmt == \"parquet\":\n",
    "        sdf = spark.read.parquet(parquet_data_path)\n",
    "    elif fmt == \"orc\":\n",
    "        sdf = spark.read.orc(orc_data_path)\n",
    "    sdf.show(5,False)\n",
    "    stats=\"{}, {}, {}\".format(fmt, \"read\", time.time() - start)\n",
    "    write_stats(stats)\n",
    "    print(stats)\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 The get_shape function prints the shape(e.g. row number and column number) of the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(df,fmt):\n",
    "    start = time.time()\n",
    "    row_num=df.count()\n",
    "    col_num=len(df.columns)\n",
    "    stats=\"{}, {}, {}\".format(fmt, \"get_shape\", time.time() - start)\n",
    "    write_stats(stats)\n",
    "    print(\"The data frame has {} rows and {} columns\".format(row_num,col_num))\n",
    "    print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 The stats function prints the min, max and numbers of a column of the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(df,fmt, field=\"rating\"):\n",
    "    start = time.time()\n",
    "    max=df.agg({field: \"max\"})\n",
    "    min=df.agg({field: \"min\"})\n",
    "    count=df.agg({field: \"count\"})\n",
    "    min.show(5,False)\n",
    "    max.show(5,False)\n",
    "    count.show(5,False)\n",
    "    stats=\"{}, {}, {}\".format(fmt, \"stats\", time.time() - start)\n",
    "    write_stats(stats)\n",
    "    print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 The random_batch function randomly select rows from the data frame. It can evaluate the ability of random data lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(df,fmt):\n",
    "    start = time.time()\n",
    "    result=df.sample(False, 0.05).collect()\n",
    "    stats=\"{}, {}, {}\".format(fmt, \"random_batch\", time.time() - start)\n",
    "    write_stats(stats)\n",
    "    print(stats)\n",
    "   # return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 The distinct function count distinct rows of the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct(df,fmt):\n",
    "    start = time.time()\n",
    "    result = df.distinct().count()\n",
    "    stats=\"{}, {}, {}\".format(fmt, \"distinct\", time.time() - start)\n",
    "    write_stats(stats)\n",
    "    print(stats)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.6 The group_by function group and count the data frame by a specific column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by(df,fmt):\n",
    "    start = time.time()\n",
    "    result=df.groupBy(\"rating\").count()\n",
    "    result.show(5,False)\n",
    "    stats=\"{}, {}, {}\".format(fmt, \"group_by\", time.time() - start)\n",
    "    write_stats(stats)\n",
    "    print(stats)\n",
    "    #return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.7 The filtering function filter data by using a specific boolean condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering(df, fmt, date=\"2005-11-15\"):\n",
    "    start = time.time()\n",
    "    result = df.filter(df.date > date).count()\n",
    "    stats=\"{}, {}, {}\".format(fmt, \"filtering\", time.time() - start)\n",
    "    write_stats(stats)\n",
    "    print(stats)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.8 The remove_space function removes space in string of a column to avoid filtering fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import ltrim,rtrim,trim\n",
    "def remove_space(df,col_name,position):\n",
    "    # remove left side space\n",
    "    if position ==\"l\":\n",
    "        return df.withColumn(\"tmp\",ltrim(col(col_name))).drop(col_name).withColumnRenamed(\"tmp\", col_name)\n",
    "    # remove right side space\n",
    "    elif position ==\"r\":\n",
    "        return df.withColumn(\"tmp\",rtrim(col(col_name))).drop(col_name).withColumnRenamed(\"tmp\", col_name)\n",
    "    # remove all side space\n",
    "    elif position ==\"a\":\n",
    "        return df.withColumn(\"tmp\",trim(col(col_name))).drop(col_name).withColumnRenamed(\"tmp\", col_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gathering stats of each data format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Get CSV format evaluation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+\n",
      "|user_id|rating|date      |\n",
      "+-------+------+----------+\n",
      "|1488844|3     |2005-09-06|\n",
      "|822109 |5     |2005-05-13|\n",
      "|885013 |4     |2005-10-19|\n",
      "|30878  |4     |2005-12-26|\n",
      "|823519 |3     |2004-05-03|\n",
      "+-------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "csv, read, 8.758424282073975\n"
     ]
    }
   ],
   "source": [
    "csv_df=read(\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data frame has 24058262 rows and 3 columns\n",
      "csv, get_shape, 6.950009822845459\n"
     ]
    }
   ],
   "source": [
    "get_shape(csv_df,\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|min(rating)|\n",
      "+-----------+\n",
      "|1          |\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|max(rating)|\n",
      "+-----------+\n",
      "|5          |\n",
      "+-----------+\n",
      "\n",
      "+-------------+\n",
      "|count(rating)|\n",
      "+-------------+\n",
      "|24053764     |\n",
      "+-------------+\n",
      "\n",
      "csv, stats, 22.299869537353516\n"
     ]
    }
   ],
   "source": [
    "# get min, max and row number of column rating\n",
    "stats(csv_df,\"csv\",field=\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv, random_batch, 15.357799291610718\n"
     ]
    }
   ],
   "source": [
    "random_batch(csv_df,\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv, distinct, 19.20894479751587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12168704"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct(csv_df,\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|rating|count  |\n",
      "+------+-------+\n",
      "|3     |6904181|\n",
      "|null  |4498   |\n",
      "|5     |5506583|\n",
      "|1     |1118186|\n",
      "|4     |8085741|\n",
      "+------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "csv, group_by, 8.280380249023438\n"
     ]
    }
   ],
   "source": [
    "group_by(csv_df,\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering(csv_df,\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=\"netflix\"\n",
    "csv_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"{}.csv\".format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Get Json format evaluation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------+\n",
      "|date      |rating|user_id|\n",
      "+----------+------+-------+\n",
      "|2005-09-06|3     |1488844|\n",
      "|2005-05-13|5     |822109 |\n",
      "|2005-10-19|4     |885013 |\n",
      "|2005-12-26|4     |30878  |\n",
      "|2004-05-03|3     |823519 |\n",
      "+----------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "json, read, 9.966949701309204\n"
     ]
    }
   ],
   "source": [
    "json_df=read(\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_shape(json_df,\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|min(rating)|\n",
      "+-----------+\n",
      "|1          |\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|max(rating)|\n",
      "+-----------+\n",
      "|5          |\n",
      "+-----------+\n",
      "\n",
      "+-------------+\n",
      "|count(rating)|\n",
      "+-------------+\n",
      "|24053764     |\n",
      "+-------------+\n",
      "\n",
      "json, stats, 35.458019971847534\n"
     ]
    }
   ],
   "source": [
    "# get min, max and row number of column rating\n",
    "stats(json_df,\"json\",field=\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json, random_batch, 17.13827896118164\n"
     ]
    }
   ],
   "source": [
    "random_batch(json_df,\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json, distinct, 21.003305673599243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12168704"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct(json_df,\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|rating|count  |\n",
      "+------+-------+\n",
      "|3     |6904181|\n",
      "|null  |4498   |\n",
      "|5     |5506583|\n",
      "|1     |1118186|\n",
      "|4     |8085741|\n",
      "+------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "json, group_by, 12.402800798416138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[rating: string, count: bigint]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_by(json_df,\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json, filtering, 11.020655632019043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "850269"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtering(json_df,\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Get Avro format evaluation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o196.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 9) (10.233.118.242 executor 5): java.lang.ClassCastException: cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.dataReader$1 of type scala.Function1 in instance of org.apache.spark.sql.execution.datasources.FileFormat$$anon$1\n\tat java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2301)\n\tat java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1431)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2411)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1184)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2296)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1184)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2296)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassCastException: cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.dataReader$1 of type scala.Function1 in instance of org.apache.spark.sql.execution.datasources.FileFormat$$anon$1\n\tat java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2301)\n\tat java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1431)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2411)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1184)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2296)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1184)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2296)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-f5228f4256b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mavro_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"avro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-94fece7aba15>\u001b[0m in \u001b[0;36mread\u001b[0;34m(fmt)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"orc\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0msdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morc_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0msdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"{}, {}, {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mwrite_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o196.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 9) (10.233.118.242 executor 5): java.lang.ClassCastException: cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.dataReader$1 of type scala.Function1 in instance of org.apache.spark.sql.execution.datasources.FileFormat$$anon$1\n\tat java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2301)\n\tat java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1431)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2411)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1184)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2296)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1184)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2296)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassCastException: cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.dataReader$1 of type scala.Function1 in instance of org.apache.spark.sql.execution.datasources.FileFormat$$anon$1\n\tat java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2301)\n\tat java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1431)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2411)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1184)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2296)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:488)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1184)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2296)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "avro_df=read(\"avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_shape(avro_df,\"avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats(avro_df,\"avro\",field=\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_batch(avro_df,\"avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct(avro_df,\"avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by(avro_df,\"avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering(avro_df,\"avro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Get Parquet format evaluation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+\n",
      "|user_id|rating|date      |\n",
      "+-------+------+----------+\n",
      "|1488844|3     |2005-09-06|\n",
      "|822109 |5     |2005-05-13|\n",
      "|885013 |4     |2005-10-19|\n",
      "|30878  |4     |2005-12-26|\n",
      "|823519 |3     |2004-05-03|\n",
      "+-------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "parquet, read, 1.6400139331817627\n"
     ]
    }
   ],
   "source": [
    "parquet_df=read(\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data frame has 24058262 rows and 3 columns\n",
      "parquet, get_shape, 1.010782241821289\n"
     ]
    }
   ],
   "source": [
    "get_shape(parquet_df,\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet, random_batch, 6.159161567687988\n"
     ]
    }
   ],
   "source": [
    "random_batch(parquet_df,\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|min(rating)|\n",
      "+-----------+\n",
      "|1          |\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|max(rating)|\n",
      "+-----------+\n",
      "|5          |\n",
      "+-----------+\n",
      "\n",
      "+-------------+\n",
      "|count(rating)|\n",
      "+-------------+\n",
      "|24053764     |\n",
      "+-------------+\n",
      "\n",
      "parquet, stats, 4.589794635772705\n"
     ]
    }
   ],
   "source": [
    "stats(parquet_df,\"parquet\",field=\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet, distinct, 164.0264663696289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12168704"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct(parquet_df,\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|rating|count  |\n",
      "+------+-------+\n",
      "|3     |6904181|\n",
      "|null  |4498   |\n",
      "|5     |5506583|\n",
      "|1     |1118186|\n",
      "|4     |8085741|\n",
      "+------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "parquet, group_by, 1.5566697120666504\n"
     ]
    }
   ],
   "source": [
    "group_by(parquet_df,\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet, filtering, 1.3744680881500244\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "850269"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtering(parquet_df,\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Get ORC format evaluation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+\n",
      "|user_id|rating|date      |\n",
      "+-------+------+----------+\n",
      "|1488844|3     |2005-09-06|\n",
      "|822109 |5     |2005-05-13|\n",
      "|885013 |4     |2005-10-19|\n",
      "|30878  |4     |2005-12-26|\n",
      "|823519 |3     |2004-05-03|\n",
      "+-------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "orc, read, 2.3085367679595947\n"
     ]
    }
   ],
   "source": [
    "orc_df=read(\"orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data frame has 24058262 rows and 3 columns\n",
      "orc, get_shape, 1.4532253742218018\n"
     ]
    }
   ],
   "source": [
    "get_shape(orc_df,\"orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orc, random_batch, 6.4762678146362305\n"
     ]
    }
   ],
   "source": [
    "random_batch(orc_df,\"orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|min(rating)|\n",
      "+-----------+\n",
      "|1          |\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|max(rating)|\n",
      "+-----------+\n",
      "|5          |\n",
      "+-----------+\n",
      "\n",
      "+-------------+\n",
      "|count(rating)|\n",
      "+-------------+\n",
      "|24053764     |\n",
      "+-------------+\n",
      "\n",
      "orc, stats, 4.612210035324097\n"
     ]
    }
   ],
   "source": [
    "stats(orc_df,\"orc\",field=\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orc, distinct, 185.58755350112915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12168704"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct(orc_df,\"orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|rating|count  |\n",
      "+------+-------+\n",
      "|3     |6904181|\n",
      "|null  |4498   |\n",
      "|5     |5506583|\n",
      "|1     |1118186|\n",
      "|4     |8085741|\n",
      "+------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "orc, group_by, 1.680478811264038\n"
     ]
    }
   ],
   "source": [
    "group_by(orc_df,\"orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orc, filtering, 1.186652421951294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "850269"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtering(orc_df,\"orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluate the performence of different data formats "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Disk Usage for different  file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType,StringType,LongType\n",
    "\n",
    "# define a schema\n",
    "\n",
    "op_schema = StructType([\n",
    "    StructField(\"format\", StringType(), True),\n",
    "    StructField(\"command\", StringType(), True),\n",
    "    StructField(\"Latency\", DoubleType(), True)])\n",
    "\n",
    "size_schema = StructType([\n",
    "    StructField(\"format\", StringType(), True),\n",
    "    StructField(\"size_kb\", LongType(), True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fed25dd5b30c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read size stats file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrawSizeDf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_format_size_stats_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_schema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# generate a new column to make data humain readable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msizeDf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrawSizeDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"size_mb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"size_kb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# read size stats file\n",
    "\n",
    "rawSizeDf=spark.read.option(\"header\", \"true\").csv(data_format_size_stats_path,schema=size_schema)\n",
    "# generate a new column to make data humain readable\n",
    "sizeDf=rawSizeDf.withColumn(\"size_mb\",col(\"size_kb\")/1024)\n",
    "sizeDf.orderBy(col(\"size_kb\").asc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert spark df to pandas df\n",
    "pd_size_df=sizeDf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the stats in a catplot\n",
    "g_size = sns.catplot(\n",
    "    data=pd_size_df, kind=\"bar\",\n",
    "    x=\"format\", y=\"size_mb\", \n",
    "    order=['orc','parquet','avro','csv','json'],\n",
    "    ci=\"sd\", palette=\"dark\", alpha=0.8, height=8\n",
    ")\n",
    "g_size.despine(left=False)\n",
    "g_size.set_axis_labels(\"\", \"Size (kb) \")\n",
    "g_size.set(ylim=(150, 1300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective of any file is to store data. It is necessary to store large volumes of different data types with less spaces. Because the disk usage can cost you a lot of money.\n",
    "\n",
    "Based on the above graphe, we can have the obvious conclusion **do not use JSON or csv to store the raw data, and orc, parquet are the best solution for storing data.Now we want to know why?** \n",
    "\n",
    "Note that we used the default configuration for all format during the entire benchmark. So we do not specify any compression codec anywhere. As a result, we have the following table\n",
    "\n",
    "| format | size(mb) | compression |\n",
    "|--------|----------|-------------|\n",
    "| json   | 1261.90  | None        |\n",
    "| csv    | 472.10   | None        |\n",
    "| avro   | 279.91   | None        |\n",
    "| parquet| 198.01   | SNAPPY      |\n",
    "| orc    | 186.84   | ZLIB        |\n",
    "\n",
    "\n",
    "1. JSON uses the most space, because it has huge overhead on storing \"schema\". Each row must repeat the column name value, which are the same for all the rows (in our example, it represents 24058262 rows). \n",
    "\n",
    "2. CSV only has one line header for storing column names, so it uses much less space than JSON  \n",
    "3. Avro does not do any compression, but it still uses less space than CSV. Because it saves raw data in binary. And the binary codec do a bit compression with storing int and long leveraging variable-length zig-zag coding.\n",
    "4. Parque alson stores raw data in binary, and it does compress data. In spark, it uses SNAPPY as the default compressing codec.\n",
    "5. Orc uses ZLIB as compression codec. As ZLIB can offer better compression ratio, so Orc use less space than Parquet. But Snappy is faster. \n",
    "\n",
    "\n",
    "You could argu that, you can gzip CSV and json which will reduce the disk usage. But, json and csv are not splitable after compression. As a result, we can not benefit from the parallel processing of Spark anymore. Thus, the advantage of avro, parquet and orc is obvious.\n",
    "\n",
    "Note that avro, parquet and orc supports many compression codec. Here we only benchmark the default setting of each format. So the difference of disk usage between these three are negligible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Visualize the data processing latency for each format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will examine the latency of several the most common data processing operations to determine which format is more optimal in certain context. We will first examine the common operations one by one. At last, we will give you an overview and conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# read stats file\n",
    "rawDf=spark.read.option(\"header\", \"false\").csv(data_format_op_stats_path,schema=op_schema)\n",
    "rawDf.show(5)\n",
    "rawDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicated rows\n",
    "tmpDf=rawDf.dropDuplicates([\"format\",\"command\"])\n",
    "# remove space in command column\n",
    "statsDf=remove_space(tmpDf,\"command\",\"l\")\n",
    "statsDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas df\n",
    "pd_df=statsDf.toPandas()\n",
    "pd_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8d415dc795c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mread_op_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mread_op_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd_df' is not defined"
     ]
    }
   ],
   "source": [
    "read_op_df=pd_df[pd_df.command.eq(\"read\")]\n",
    "read_op_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the stats in a catplot\n",
    "g_read_op = sns.catplot(\n",
    "    data=read_op_df, kind=\"bar\",\n",
    "    x=\"format\", y=\"Latency\", \n",
    "    order=['orc','parquet','avro','csv','json'],\n",
    "    ci=\"sd\", palette=\"dark\", alpha=0.8, height=8\n",
    ")\n",
    "g_read_op.despine(left=False)\n",
    "g_read_op.set_axis_labels(\"\", \"Latency (seconds) \")\n",
    "g_read_op.set(ylim=(0, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could notice that the reading speed of orc, parquet and avro are much faster than csv and json. Because they store raw data in binary, which are optimized for performence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Get basic stats such as min, max, column numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic stats of a data set such as min, max, row numbers are the basic information we will gather when we exploring un data set. Below figure shows the latency of each format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_op_df=pd_df.loc[(pd_df.command.eq(\"stats\"))|(pd_df.command.eq(\"get_shape\"))]\n",
    "meta_op_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_op_meta = sns.catplot(\n",
    "    data=meta_op_df, kind=\"bar\",\n",
    "    x=\"format\", y=\"Latency\", hue=\"command\",\n",
    "    order=['orc','parquet','avro','csv','json'],\n",
    "    ci=\"sd\", palette=\"dark\", alpha=0.8, height=10\n",
    ")\n",
    "g_op_meta.despine(left=False)\n",
    "g_op_meta.set_axis_labels(\"\", \"Latency (seconds)\")\n",
    "g_op_meta.legend.set_title(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orc and Parquet saves the min, max and row numbers as metadata. So the basic stats operations do not require the process of the entire dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Random batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random batch select randomly a subset of rows in a dataframe. If checks the performence of radom access operations of a data format. Unlike sequential access, random access requires more sophisticated optimization to avoid read unnecessary data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_op_df=pd_df[pd_df.command.eq(\"random_batch\")]\n",
    "random_op_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the stats in a catplot\n",
    "g_random_op = sns.catplot(\n",
    "    data=random_op_df, kind=\"bar\",\n",
    "    x=\"format\", y=\"Latency\", \n",
    "    order=['orc','parquet','avro','csv','json'],\n",
    "    ci=\"sd\", palette=\"dark\", alpha=0.8, height=8\n",
    ")\n",
    "g_random_op.despine(left=False)\n",
    "g_random_op.set_axis_labels(\"\", \"Latency (seconds) \")\n",
    "g_random_op.set(ylim=(0, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "orc, parquet, and avro split data in small chunks, and each chunk has a header which contains the metadata of the data in this chunk. As a result, when a certain row is required, the reader will check first the metadata in the header, if the condition is not matched, this chunk will be omitted. And this will avoid to read all unnecessary data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 column-wise operations (Filtering/GroupBy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many operations such as filtering or groupBy only instrested in data of certain columns. If we can avoid reading unnecessary data. We can improve dramatiquely the performence. Below figure shows the latency of filtering and groupBy operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_op_df=pd_df.loc[(pd_df.command.eq(\"group_by\"))|(pd_df.command.eq(\"filtering\"))]\n",
    "col_op_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_col_op = sns.catplot(\n",
    "    data=col_op_df, kind=\"bar\",hue=\"command\",\n",
    "    x=\"format\", y=\"Latency\", \n",
    "    order=['orc','parquet','avro','csv','json'],\n",
    "    ci=\"sd\", palette=\"dark\", alpha=0.8, height=8\n",
    ")\n",
    "g_col_op.despine(left=False)\n",
    "g_col_op.set_axis_labels(\"\", \"Latency (seconds) \")\n",
    "g_col_op.set(ylim=(0, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice orc and parquet are much quicker than avro, csv, and json. Because orc, and parquet are columnar-based data formats, and avro, csv, and json are row-based data formats. We will discuss the difference between columnar-based and row-based in another tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5.row-wise operation (Distinct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_op_df=pd_df[pd_df.command.eq(\"distinct\")]\n",
    "distinct_op_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_distinct_op = sns.catplot(\n",
    "    data=distinct_op_df, kind=\"bar\",\n",
    "    x=\"format\", y=\"Latency\", \n",
    "    order=['orc','parquet','avro','csv','json'],\n",
    "    ci=\"sd\", palette=\"dark\", alpha=0.8, height=8\n",
    ")\n",
    "g_distinct_op.despine(left=False)\n",
    "g_distinct_op.set_axis_labels(\"\", \"Latency (seconds) \")\n",
    "g_distinct_op.set(ylim=(0, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Overview and Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_op = sns.catplot(\n",
    "    data=pd_df, kind=\"bar\",\n",
    "    x=\"format\", y=\"Latency\", hue=\"command\",\n",
    "    order=['orc','parquet','avro','csv','json'],\n",
    "    ci=\"sd\", palette=\"dark\", alpha=0.8, height=10\n",
    ")\n",
    "g_op.despine(left=False)\n",
    "g_op.set_axis_labels(\"\", \"Latency (seconds)\")\n",
    "g_op.legend.set_title(\" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure shows the overall operation latency of all data formats. You can easily spot the most appropriate format for you based on your use case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Some basic properties of data format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen the operation latency of all data formats. But there are other properties that may impact the choice of the data format. The below table shows some basic properties of data formats when we evaluate them. \n",
    "\n",
    "\n",
    "|Property |CSV |Json|Parquet|Avro|ORC|\n",
    "|---------|----|----|-------|----|---|\n",
    "|Human Readable|YES|YES|NO|NO|NO|\n",
    "|Compressable|YES|YES|YES|YES|YES|\n",
    "|Splittable|YES*|YES*|YES|YES|YES|\n",
    "|Complex data structure|NO|YES|YES|YES|YES|\n",
    "|Schema evolution|NO|NO|YES|YES|YES|\n",
    "|Columnar|NO|NO|YES|NO|YES|\n",
    "\n",
    "Note:\n",
    "\n",
    "1. CSV is splittable when it is a raw, uncompressed file or using a splittable compression format such as BZIP2 or LZO (note: LZO needs to be indexed to be splittable!)\n",
    "2. JSON has the same conditions about splittability when compressed as CSV with one extra difference. When wholeFile option is set to true in Spark(re: SPARK-18352), JSON is NOT splittable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Best data format for OLAP: Parquet vs Orc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the above analysis, we can say that Orc and Parquet are the best data formats for OLAP applications. They both support various compression algorithms which reduce significantly disk usage. They are both very efficient on columnar-oriented data analysis operations. \n",
    "\n",
    "Parquet has better support on nested data types than Orc. Orc loses compression ratio and analysis performance when data contains complex nested data types.\n",
    "\n",
    "Orc supports data update and ACID (atomicity, consistency, isolation, durability). Parquet does not, so if you want to update a Parquet file, you need to create a new one based on the old one.\n",
    "\n",
    "Parquet has better interoperability compare to Orc. Because almost all data analysis tools and framework supports parquet. Orc is only supported by Spark, Hive, Impala, MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are building a data lake, warehouse, or mart. We strongly recommend you use Parquet as the default data format for all your data.\n",
    "If you need to constantly update(write) your data, do not use columnar-based data formats such as ORC, Parquet.\n",
    "If you need to update your data schema, use Avro.\n",
    "\n",
    "As you know, when we load data from disk to memory via a tool (Spark, Hive, Pandas, etc), the tool needs to convert the file format(disk) to object models(memory). This conversion is very expensive, and each tool provides its own object models. To avoid the conversion of object models of different tools, a standard model has been proposed(i.e. Apache Arrow). Want to know more? Stay tuned for our next tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop the spark cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop sparksession\n",
    "spark.sparkContext.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the spark cluster is well closed. You should not see any python-spark pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The connection to the server api-server.static.lab.sspcloud.fr was refused - did you specify the right host or port?\n"
     ]
    }
   ],
   "source": [
    "! kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
