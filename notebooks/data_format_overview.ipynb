{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data format overview\n",
    "## In this tutorial, we will overview evaluate the following data formats\n",
    "1. avro (structured)\n",
    "2. csv (semi-structured)\n",
    "3. json (semi-structured)\n",
    "4. orc (structured)\n",
    "5. parquet (structured) \n",
    "\n",
    "## \n",
    "for their disk usage, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Key': 'pengfei/pengfei_test',\n",
       " 'name': 'pengfei/pengfei_test',\n",
       " 'type': 'directory',\n",
       " 'Size': 0,\n",
       " 'size': 0,\n",
       " 'StorageClass': 'DIRECTORY'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import s3fs\n",
    "endpoint = \"https://\"+os.environ['AWS_S3_ENDPOINT']\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': endpoint})\n",
    "event_log_path=\"pengfei/spark-history\"\n",
    "\n",
    "fs.touch('s3://'+event_log_path+'/.keep')\n",
    "fs.info('pengfei/pengfei_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "       .builder.master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "       .appName(\"Python Spark SQL basic example\") \\\n",
    "       .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:master\") \\\n",
    "       .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "       .config(\"spark.executor.instances\", \"5\") \\\n",
    "       .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "       .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "       .config(\"spark.eventLog.dir\",\"s3a://\"+event_log_path) \\\n",
    "       .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.0.1\") \\\n",
    "       .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data_path=\"s3a://pengfei/sspcloud-demo/data_format/netflix.json\"\n",
    "parquet_data_path=\"s3a://pengfei/sspcloud-demo/data_format/netflix.parquet\"\n",
    "avro_data_path=\"s3a://pengfei/sspcloud-demo/data_format/netflix.avro\"\n",
    "orc_data_path=\"s3a://pengfei/sspcloud-demo/data_format/netflix.orc\"\n",
    "csv_data_path=\"s3a://pengfei/sspcloud-demo/data_format/netflix.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The read function read the source data file and convert it to a spark data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def read(fmt):\n",
    "    start = time.time()\n",
    "    if fmt == \"json\":\n",
    "        sdf = spark.read.option(\"header\", \"true\").json(json_data_path)\n",
    "    elif fmt == \"csv\":\n",
    "        sdf = spark.read.option(\"header\", \"true\").csv(csv_data_path)\n",
    "    elif fmt == \"avro\":\n",
    "        sdf = spark.read.format(\"avro\").option(\"header\", \"true\").load(avro_data_path)\n",
    "    elif fmt == \"parquet\":\n",
    "        sdf = spark.read.option(\"header\", \"true\").parquet(parquet_data_path)\n",
    "    elif fmt == \"orc\":\n",
    "        sdf = spark.read.read.orc(orc_data_path)\n",
    "    sdf.show(5,False)\n",
    "    print(\"{}, {}, {}\".format(fmt, \"read\", time.time() - start))\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The get_shape function prints the shape(e.g. row number and column number) of the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(df,fmt):\n",
    "    start = time.time()\n",
    "    row_num=df.count()\n",
    "    col_num=len(df.columns)\n",
    "    print(\"The data frame has {} rows and {} columns\".format(row_num,col_num))\n",
    "    print(\"{}, {}, {}\".format(fmt, \"get_shape\", time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The stats function prints the min, max and numbers of a column of the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(df,fmt, field=\"rating\"):\n",
    "    start = time.time()\n",
    "    max=df.agg({field: \"max\"})\n",
    "    min=df.agg({field: \"min\"})\n",
    "    count=df.agg({field: \"count\"})\n",
    "    print(\"{}, {}, {}\".format(fmt, \"random_batch\", time.time() - start))\n",
    "    min.show(5,False)\n",
    "    max.show(5,False)\n",
    "    count.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The random_batch function randomly select rows from the data frame. It can evaluate the ability of random data lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(df,fmt):\n",
    "    start = time.time()\n",
    "    result=df.sample(False, 0.05).collect()\n",
    "    print(\"{}, {}, {}\".format(fmt, \"random_batch\", time.time() - start))\n",
    "   # return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct(df,fmt):\n",
    "    start = time.time()\n",
    "    result = df.distinct().count()\n",
    "    print(\"{}, {}, {}\".format(fmt, \"distinct\", time.time() - start))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by(df,fmt):\n",
    "    start = time.time()\n",
    "    result=df.groupBy(\"rating\").count()\n",
    "    print(\"{}, {}, {}\".format(fmt, \"group_by\", time.time() - start))\n",
    "    result.show(5,False)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering(df, fmt, date=\"2005-11-15\"):\n",
    "    start = time.time()\n",
    "    result = df.filter(df.date > date).count()\n",
    "    print(\"{}, {}, {}\".format(fmt, \"filtering\", time.time() - start))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Json format evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df=read(\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data frame has 24058262 rows and 3 columns\n",
      "json, get_shape, 8.26310682296753\n"
     ]
    }
   ],
   "source": [
    "get_shape(json_df,\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json, random_batch, 15.675529956817627\n"
     ]
    }
   ],
   "source": [
    "random_batch(json_df,\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct(json_df,\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json, filtering, 10.1075279712677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "850269"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtering(json_df,\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avro format evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_df=read(\"avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet format evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+\n",
      "|user_id|rating|date      |\n",
      "+-------+------+----------+\n",
      "|1488844|3     |2005-09-06|\n",
      "|822109 |5     |2005-05-13|\n",
      "|885013 |4     |2005-10-19|\n",
      "|30878  |4     |2005-12-26|\n",
      "|823519 |3     |2004-05-03|\n",
      "+-------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "parquet, read, 2.3800978660583496\n"
     ]
    }
   ],
   "source": [
    "parquet_df=read(\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data frame has 24058262 rows and 3 columns\n",
      "parquet, get_shape, 1.3256816864013672\n"
     ]
    }
   ],
   "source": [
    "get_shape(parquet_df,\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet, random_batch, 7.559308767318726\n"
     ]
    }
   ],
   "source": [
    "random_batch(parquet_df,\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet, random_batch, 0.03509926795959473\n",
      "+-----------+\n",
      "|min(rating)|\n",
      "+-----------+\n",
      "|1          |\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|max(rating)|\n",
      "+-----------+\n",
      "|5          |\n",
      "+-----------+\n",
      "\n",
      "+-------------+\n",
      "|count(rating)|\n",
      "+-------------+\n",
      "|24053764     |\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stats(parquet_df,\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet, group_by, 0.015201091766357422\n",
      "+------+-------+\n",
      "|rating|count  |\n",
      "+------+-------+\n",
      "|3     |6904181|\n",
      "|null  |4498   |\n",
      "|5     |5506583|\n",
      "|1     |1118186|\n",
      "|4     |8085741|\n",
      "+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[rating: string, count: bigint]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_by(parquet_df,\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop sparksession\n",
    "spark.sparkContext.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                     READY   STATUS      RESTARTS   AGE\n",
      "deleting-pods-with-completed-status-1615298400-jk8c6     0/1     Completed   0          51m\n",
      "jupyter-1615290850-69d9fbd8d6-rlwhs                      1/1     Running     0          177m\n",
      "python-spark-sql-basic-example-ca8c007817777a02-exec-1   1/1     Running     0          84s\n",
      "python-spark-sql-basic-example-ca8c007817777a02-exec-2   1/1     Running     0          84s\n",
      "python-spark-sql-basic-example-ca8c007817777a02-exec-3   1/1     Running     0          84s\n",
      "python-spark-sql-basic-example-ca8c007817777a02-exec-4   1/1     Running     0          84s\n",
      "python-spark-sql-basic-example-ca8c007817777a02-exec-5   1/1     Running     0          84s\n",
      "ubuntu-1612965548-79c9567b44-nhs9p                       1/1     Running     0          27d\n"
     ]
    }
   ],
   "source": [
    "! kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
