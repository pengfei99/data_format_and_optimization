{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: spark-submit [options] <app jar | python file | R file> [app arguments]\n",
      "Usage: spark-submit --kill [submission ID] --master [spark://...]\n",
      "Usage: spark-submit --status [submission ID] --master [spark://...]\n",
      "Usage: spark-submit run-example [options] example-class [example args]\n",
      "\n",
      "Options:\n",
      "  --master MASTER_URL         spark://host:port, mesos://host:port, yarn,\n",
      "                              k8s://https://host:port, or local (Default: local[*]).\n",
      "  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (\"client\") or\n",
      "                              on one of the worker machines inside the cluster (\"cluster\")\n",
      "                              (Default: client).\n",
      "  --class CLASS_NAME          Your application's main class (for Java / Scala apps).\n",
      "  --name NAME                 A name of your application.\n",
      "  --jars JARS                 Comma-separated list of jars to include on the driver\n",
      "                              and executor classpaths.\n",
      "  --packages                  Comma-separated list of maven coordinates of jars to include\n",
      "                              on the driver and executor classpaths. Will search the local\n",
      "                              maven repo, then maven central and any additional remote\n",
      "                              repositories given by --repositories. The format for the\n",
      "                              coordinates should be groupId:artifactId:version.\n",
      "  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while\n",
      "                              resolving the dependencies provided in --packages to avoid\n",
      "                              dependency conflicts.\n",
      "  --repositories              Comma-separated list of additional remote repositories to\n",
      "                              search for the maven coordinates given with --packages.\n",
      "  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place\n",
      "                              on the PYTHONPATH for Python apps.\n",
      "  --files FILES               Comma-separated list of files to be placed in the working\n",
      "                              directory of each executor. File paths of these files\n",
      "                              in executors can be accessed via SparkFiles.get(fileName).\n",
      "\n",
      "  --conf, -c PROP=VALUE       Arbitrary Spark configuration property.\n",
      "  --properties-file FILE      Path to a file from which to load extra properties. If not\n",
      "                              specified, this will look for conf/spark-defaults.conf.\n",
      "\n",
      "  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).\n",
      "  --driver-java-options       Extra Java options to pass to the driver.\n",
      "  --driver-library-path       Extra library path entries to pass to the driver.\n",
      "  --driver-class-path         Extra class path entries to pass to the driver. Note that\n",
      "                              jars added with --jars are automatically included in the\n",
      "                              classpath.\n",
      "\n",
      "  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).\n",
      "\n",
      "  --proxy-user NAME           User to impersonate when submitting the application.\n",
      "                              This argument does not work with --principal / --keytab.\n",
      "\n",
      "  --help, -h                  Show this help message and exit.\n",
      "  --verbose, -v               Print additional debug output.\n",
      "  --version,                  Print the version of current Spark.\n",
      "\n",
      " Cluster deploy mode only:\n",
      "  --driver-cores NUM          Number of cores used by the driver, only in cluster mode\n",
      "                              (Default: 1).\n",
      "\n",
      " Spark standalone or Mesos with cluster deploy mode only:\n",
      "  --supervise                 If given, restarts the driver on failure.\n",
      "\n",
      " Spark standalone, Mesos or K8s with cluster deploy mode only:\n",
      "  --kill SUBMISSION_ID        If given, kills the driver specified.\n",
      "  --status SUBMISSION_ID      If given, requests the status of the driver specified.\n",
      "\n",
      " Spark standalone, Mesos and Kubernetes only:\n",
      "  --total-executor-cores NUM  Total cores for all executors.\n",
      "\n",
      " Spark standalone, YARN and Kubernetes only:\n",
      "  --executor-cores NUM        Number of cores used by each executor. (Default: 1 in\n",
      "                              YARN and K8S modes, or all available cores on the worker\n",
      "                              in standalone mode).\n",
      "\n",
      " Spark on YARN and Kubernetes only:\n",
      "  --num-executors NUM         Number of executors to launch (Default: 2).\n",
      "                              If dynamic allocation is enabled, the initial number of\n",
      "                              executors will be at least NUM.\n",
      "  --principal PRINCIPAL       Principal to be used to login to KDC.\n",
      "  --keytab KEYTAB             The full path to the file that contains the keytab for the\n",
      "                              principal specified above.\n",
      "\n",
      " Spark on YARN only:\n",
      "  --queue QUEUE_NAME          The YARN queue to submit to (Default: \"default\").\n",
      "  --archives ARCHIVES         Comma separated list of archives to be extracted into the\n",
      "                              working directory of each executor.\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "! /opt/spark/bin/spark-submit --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Key': 'pengfei/pengfei_test',\n",
       " 'name': 'pengfei/pengfei_test',\n",
       " 'type': 'directory',\n",
       " 'Size': 0,\n",
       " 'size': 0,\n",
       " 'StorageClass': 'DIRECTORY'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import s3fs\n",
    "endpoint = \"https://\"+os.environ['AWS_S3_ENDPOINT']\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': endpoint})\n",
    "\n",
    "event_log_path=\"pengfei/spark-history\"\n",
    "\n",
    "\n",
    "fs.touch('s3://'+event_log_path+'/.keep')\n",
    "fs.info('pengfei/pengfei_test')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:master\") \n",
    "conf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT']) \n",
    "conf.set(\"spark.executor.instances\", \"5\")\n",
    "conf.set(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \n",
    "# save event log, can be ommitted\n",
    "conf.set(\"spark.eventLog.enabled\",\"true\")\n",
    "conf.set(\"spark.eventLog.dir\",\"s3a://\"+event_log_path)\n",
    "spark = SparkSession.builder.master(\"k8s://https://kubernetes.default.svc:443\").appName(\"Python Spark SQL basic example\").config(conf).getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                   READY   STATUS        RESTARTS   AGE\n",
      "deleting-pods-with-completed-status-1614949200-5hz5c   0/1     Completed     0          2m23s\n",
      "jupyter-1614586885-846767d94b-r5zx5                    1/1     Running       0          4d4h\n",
      "jupyter-1614938651-6cdc6ff6d5-494jt                    1/1     Running       0          178m\n",
      "pyspark-shell-ac03c9780279915c-exec-5                  0/1     Terminating   0          2m6s\n",
      "pyspark-shell-ced9457801ec11a7-exec-1                  1/1     Running       0          156m\n",
      "pyspark-shell-ced9457801ec11a7-exec-2                  1/1     Running       0          156m\n",
      "pyspark-shell-ced9457801ec11a7-exec-3                  1/1     Running       0          156m\n",
      "pyspark-shell-ced9457801ec11a7-exec-4                  1/1     Running       0          156m\n",
      "pyspark-shell-ced9457801ec11a7-exec-5                  1/1     Running       0          156m\n",
      "ubuntu-1612965548-79c9567b44-nhs9p                     1/1     Running       0          22d\n"
     ]
    }
   ],
   "source": [
    "! kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute '_get_object_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-38ce654d9200>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"s3a://pengfei/sspcloud-demo/data_format/Fire_Department_Calls_for_Service.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    182\u001b[0m                         \u001b[0msparkConf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m                             \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m                         \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/conf.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# Try to set self._jconf first if JVM is created, set self._conf if JVM is not created yet.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m         \u001b[0margs_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m         \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCALL_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_build_args\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m         args_command = \"\".join(\n\u001b[0;32m-> 1266\u001b[0;31m             [get_command_part(arg, self.pool) for arg in new_args])\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m         args_command = \"\".join(\n\u001b[0;32m-> 1266\u001b[0;31m             [get_command_part(arg, self.pool) for arg in new_args])\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_command_part\u001b[0;34m(parameter, python_proxy_pool)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mcommand_part\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\";\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minterface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mcommand_part\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0mcommand_part\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute '_get_object_id'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "data_path=\"s3a://pengfei/sspcloud-demo/data_format/Fire_Department_Calls_for_Service.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.stop()\n",
    "spark.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType\n",
    "\n",
    "fireSchema = StructType([StructField('CallNumber', IntegerType(), True),\n",
    "                     StructField('UnitID', StringType(), True),\n",
    "                     StructField('IncidentNumber', IntegerType(), True),\n",
    "                     StructField('CallType', StringType(), True),                  \n",
    "                     StructField('CallDate', StringType(), True),       \n",
    "                     StructField('WatchDate', StringType(), True),       \n",
    "                     StructField('ReceivedDtTm', StringType(), True),       \n",
    "                     StructField('EntryDtTm', StringType(), True),       \n",
    "                     StructField('DispatchDtTm', StringType(), True),       \n",
    "                     StructField('ResponseDtTm', StringType(), True),       \n",
    "                     StructField('OnSceneDtTm', StringType(), True),       \n",
    "                     StructField('TransportDtTm', StringType(), True),                  \n",
    "                     StructField('HospitalDtTm', StringType(), True),       \n",
    "                     StructField('CallFinalDisposition', StringType(), True),       \n",
    "                     StructField('AvailableDtTm', StringType(), True),       \n",
    "                     StructField('Address', StringType(), True),       \n",
    "                     StructField('City', StringType(), True),       \n",
    "                     StructField('ZipcodeofIncident', IntegerType(), True),       \n",
    "                     StructField('Battalion', StringType(), True),                 \n",
    "                     StructField('StationArea', StringType(), True),       \n",
    "                     StructField('Box', StringType(), True),       \n",
    "                     StructField('OriginalPriority', StringType(), True),       \n",
    "                     StructField('Priority', StringType(), True),       \n",
    "                     StructField('FinalPriority', IntegerType(), True),       \n",
    "                     StructField('ALSUnit', BooleanType(), True),       \n",
    "                     StructField('CallTypeGroup', StringType(), True),\n",
    "                     StructField('NumberofAlarms', IntegerType(), True),\n",
    "                     StructField('UnitType', StringType(), True),\n",
    "                     StructField('Unitsequenceincalldispatch', IntegerType(), True),\n",
    "                     StructField('FirePreventionDistrict', StringType(), True),\n",
    "                     StructField('SupervisorDistrict', StringType(), True),\n",
    "                     StructField('NeighborhoodDistrict', StringType(), True),\n",
    "                     StructField('Location', StringType(), True),\n",
    "                     StructField('RowID', StringType(), True)])\n",
    "raw_data="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
