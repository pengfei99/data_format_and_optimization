{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: spark-submit [options] <app jar | python file | R file> [app arguments]\n",
      "Usage: spark-submit --kill [submission ID] --master [spark://...]\n",
      "Usage: spark-submit --status [submission ID] --master [spark://...]\n",
      "Usage: spark-submit run-example [options] example-class [example args]\n",
      "\n",
      "Options:\n",
      "  --master MASTER_URL         spark://host:port, mesos://host:port, yarn,\n",
      "                              k8s://https://host:port, or local (Default: local[*]).\n",
      "  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (\"client\") or\n",
      "                              on one of the worker machines inside the cluster (\"cluster\")\n",
      "                              (Default: client).\n",
      "  --class CLASS_NAME          Your application's main class (for Java / Scala apps).\n",
      "  --name NAME                 A name of your application.\n",
      "  --jars JARS                 Comma-separated list of jars to include on the driver\n",
      "                              and executor classpaths.\n",
      "  --packages                  Comma-separated list of maven coordinates of jars to include\n",
      "                              on the driver and executor classpaths. Will search the local\n",
      "                              maven repo, then maven central and any additional remote\n",
      "                              repositories given by --repositories. The format for the\n",
      "                              coordinates should be groupId:artifactId:version.\n",
      "  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while\n",
      "                              resolving the dependencies provided in --packages to avoid\n",
      "                              dependency conflicts.\n",
      "  --repositories              Comma-separated list of additional remote repositories to\n",
      "                              search for the maven coordinates given with --packages.\n",
      "  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place\n",
      "                              on the PYTHONPATH for Python apps.\n",
      "  --files FILES               Comma-separated list of files to be placed in the working\n",
      "                              directory of each executor. File paths of these files\n",
      "                              in executors can be accessed via SparkFiles.get(fileName).\n",
      "\n",
      "  --conf, -c PROP=VALUE       Arbitrary Spark configuration property.\n",
      "  --properties-file FILE      Path to a file from which to load extra properties. If not\n",
      "                              specified, this will look for conf/spark-defaults.conf.\n",
      "\n",
      "  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).\n",
      "  --driver-java-options       Extra Java options to pass to the driver.\n",
      "  --driver-library-path       Extra library path entries to pass to the driver.\n",
      "  --driver-class-path         Extra class path entries to pass to the driver. Note that\n",
      "                              jars added with --jars are automatically included in the\n",
      "                              classpath.\n",
      "\n",
      "  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).\n",
      "\n",
      "  --proxy-user NAME           User to impersonate when submitting the application.\n",
      "                              This argument does not work with --principal / --keytab.\n",
      "\n",
      "  --help, -h                  Show this help message and exit.\n",
      "  --verbose, -v               Print additional debug output.\n",
      "  --version,                  Print the version of current Spark.\n",
      "\n",
      " Cluster deploy mode only:\n",
      "  --driver-cores NUM          Number of cores used by the driver, only in cluster mode\n",
      "                              (Default: 1).\n",
      "\n",
      " Spark standalone or Mesos with cluster deploy mode only:\n",
      "  --supervise                 If given, restarts the driver on failure.\n",
      "\n",
      " Spark standalone, Mesos or K8s with cluster deploy mode only:\n",
      "  --kill SUBMISSION_ID        If given, kills the driver specified.\n",
      "  --status SUBMISSION_ID      If given, requests the status of the driver specified.\n",
      "\n",
      " Spark standalone, Mesos and Kubernetes only:\n",
      "  --total-executor-cores NUM  Total cores for all executors.\n",
      "\n",
      " Spark standalone, YARN and Kubernetes only:\n",
      "  --executor-cores NUM        Number of cores used by each executor. (Default: 1 in\n",
      "                              YARN and K8S modes, or all available cores on the worker\n",
      "                              in standalone mode).\n",
      "\n",
      " Spark on YARN and Kubernetes only:\n",
      "  --num-executors NUM         Number of executors to launch (Default: 2).\n",
      "                              If dynamic allocation is enabled, the initial number of\n",
      "                              executors will be at least NUM.\n",
      "  --principal PRINCIPAL       Principal to be used to login to KDC.\n",
      "  --keytab KEYTAB             The full path to the file that contains the keytab for the\n",
      "                              principal specified above.\n",
      "\n",
      " Spark on YARN only:\n",
      "  --queue QUEUE_NAME          The YARN queue to submit to (Default: \"default\").\n",
      "  --archives ARCHIVES         Comma separated list of archives to be extracted into the\n",
      "                              working directory of each executor.\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "! /opt/spark/bin/spark-submit --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Key': 'pengfei/pengfei_test',\n",
       " 'name': 'pengfei/pengfei_test',\n",
       " 'type': 'directory',\n",
       " 'Size': 0,\n",
       " 'size': 0,\n",
       " 'StorageClass': 'DIRECTORY'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import s3fs\n",
    "endpoint = \"https://\"+os.environ['AWS_S3_ENDPOINT']\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': endpoint})\n",
    "\n",
    "event_log_path=\"pengfei/spark-history\"\n",
    "\n",
    "\n",
    "fs.touch('s3://'+event_log_path+'/.keep')\n",
    "fs.info('pengfei/pengfei_test')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"k8s://https://kubernetes.default.svc:443\") \n",
    "conf.set(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:master\") \n",
    "conf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT']) \n",
    "conf.set(\"spark.executor.instances\", \"5\")\n",
    "conf.set(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \n",
    "# save event log, can be ommitted\n",
    "conf.set(\"spark.eventLog.enabled\",\"true\")\n",
    "conf.set(\"spark.eventLog.dir\",\"s3a://\"+event_log_path)\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                   READY   STATUS      RESTARTS   AGE\n",
      "deleting-pods-with-completed-status-1614938400-rmvjt   0/1     Completed   0          29m\n",
      "jupyter-1614586885-846767d94b-r5zx5                    1/1     Running     0          4d2h\n",
      "jupyter-1614938651-6cdc6ff6d5-494jt                    1/1     Running     0          25m\n",
      "jupyter-1614939372-6859cf6bff-ghljd                    1/1     Running     0          13m\n",
      "pyspark-shell-ced9457801ec11a7-exec-1                  1/1     Running     0          3m38s\n",
      "pyspark-shell-ced9457801ec11a7-exec-2                  1/1     Running     0          3m38s\n",
      "pyspark-shell-ced9457801ec11a7-exec-3                  1/1     Running     0          3m38s\n",
      "pyspark-shell-ced9457801ec11a7-exec-4                  1/1     Running     0          3m38s\n",
      "pyspark-shell-ced9457801ec11a7-exec-5                  1/1     Running     0          3m38s\n",
      "ubuntu-1612965548-79c9567b44-nhs9p                     1/1     Running     0          22d\n"
     ]
    }
   ],
   "source": [
    "! kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
